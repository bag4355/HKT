import openpyxl
from PyPDF2 import PdfReader
import concurrent.futures
import json
from typing import List
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http import models as qdrant_models
from openai import OpenAI
import re
import os

upstage_api_key = os.environ.get('upstage_api_key')
cohere_api_key = os.environ.get('cohere_api_key')
qdrant_api_key = os.environ.get('qdrant_api_key')
qdrant_host = os.environ.get('qdrant_host')
COLLECTION_NAME = os.environ.get('COLLECTION_NAME')

workbook = openpyxl.load_workbook('crawled_data.xlsx')  # Korea Accounting Law Data
sheet = workbook.active

law_list = []
for cell in sheet['C']:
    value = str(cell.value) if cell.value is not None else ""
    law_list.append(value)


os.environ.pop("HTTP_PROXY", None)
os.environ.pop("HTTPS_PROXY", None)

llm_client = OpenAI(
    api_key=upstage_api_key,
    base_url="https://api.upstage.ai/v1"
)
co = cohere.Client(cohere_api_key)
qdrant_client = QdrantClient(
    host=qdrant_host,
    port=6333,
    https=True,
    api_key=qdrant_api_key
)


def extract_text_from_pdf(pdf_path: str) -> str:
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text


def split_text_by_length(text: str, chunk_size: int = 8000) -> list:
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]


def summarize_chunk(chunk: str) -> str:
    prompt = f"""ë‹¤ìŒì€ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œì˜ ì¼ë¶€ìž…ë‹ˆë‹¤.

ì´ ë³´ê³ ì„œì—ì„œ ì•„ëž˜ í•­ëª©ë“¤ì— í•´ë‹¹í•˜ëŠ” ì¤‘ìš”í•œ íšŒê³„ ì •ë³´ê°€ ìžˆë‹¤ë©´ ê°€ëŠ¥í•œ í•œ ëª¨ë‘ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ ì£¼ì„¸ìš”.

- íšŒê³„ì •ì±…ë³€ê²½ (íšŒê³„ì •ì±… ë˜ëŠ” íšŒê³„ì¶”ì • ë³€ê²½ ë‚´ìš©)
- ìˆ˜ìµì¸ì‹_ê¸°ì¤€ (ìˆ˜ìµì„ ì–´ë–¤ ë°©ì‹/ì‹œì ìœ¼ë¡œ ì¸ì‹í•˜ëŠ”ì§€)
- ì¡°ê±´ë¶€ìˆ˜ìµ_ë³€ìˆ˜ë³´ìƒ (ì„±ê³¼ê¸‰ ë“± ì¡°ê±´ë¶€ ìˆ˜ìµ ê´€ë ¨ ì²˜ë¦¬)
- ê³„ì•½ë³€ê²½ (ê³„ì•½ ì¡°ê±´ ë³€ê²½ê³¼ íšŒê³„ì˜í–¥)
- ì§„í–‰ë¥ ê¸°ì¤€ìˆ˜ìµ (ìž¥ê¸°ê³µì‚¬ë‚˜ ìš©ì—­ì— ëŒ€í•œ ìˆ˜ìµ ì¸ì‹)
- ë¦¬ìŠ¤ë¶€ì™¸ì²˜ë¦¬ (ìžì‚°Â·ë¶€ì±„ ì¸ì‹ ì œì™¸ëœ ë¦¬ìŠ¤ ê´€ë ¨ ì‚¬ìœ )
- ì¶©ë‹¹ë¶€ì±„_ë¯¸ì¸ì‹ (ìš°ë°œì±„ë¬´ ì¡´ìž¬ì—ë„ ë¶ˆêµ¬í•˜ê³  ë¯¸ì¸ì‹ëœ ì‚¬ìœ )
- ì •ë¶€ë³´ì¡°ê¸ˆì²˜ë¦¬ (ì •ë¶€ë³´ì¡°ê¸ˆ ì¸ì‹ ë° ì²˜ë¦¬ ë°©ì‹)
- ë¬´í˜•ìžì‚°_ìžì‚°í™”ì—¬ë¶€ (ê°œë°œë¹„ ë“± ë¬´í˜•ìžì‚°ì˜ ìžì‚°í™” ì—¬ë¶€)
- ì†ìƒê²€ì‚¬ (ì˜ì—…ê¶Œ, íˆ¬ìžìžì‚° ë“±ì˜ ì†ìƒê²€ì‚¬ ì—¬ë¶€ì™€ ê¸°ì¤€)
- ê°ê°€ìƒê°ë°©ë²•_ë³€ê²½ (ê°ê°€ìƒê° ë°©ì‹ ë˜ëŠ” ë‚´ìš©ì—°ìˆ˜ ë³€ê²½ ë‚´ìš©)

ë˜í•œ ê¸°ì—…ì˜ ë°°ê²½ì„ íŒŒì•…í•  ìˆ˜ ìžˆë„ë¡ ë‹¤ìŒ ì •ë³´ë„ í¬í•¨í•´ ì£¼ì„¸ìš”:

- ì‚°ì—…ë¶„ë¥˜
- ì œí’ˆìœ í˜•
- ìˆ˜ìµêµ¬ì¡°
- ë§¤ì¶œêµ¬ì„±
- ê³ ê°ìœ í˜•
- ê³„ì•½êµ¬ì¡°
- ì—°ê²°ëŒ€ìƒì—¬ë¶€ (ì—°ê²° vs ê°œë³„ ìž¬ë¬´ì œí‘œ ê¸°ì¤€)
- íšŒê³„ê¸°ì¤€ (K-IFRS, K-GAAP ë“±)
- ìƒìž¥ì—¬ë¶€ (ìƒìž¥ or ë¹„ìƒìž¥)

â€» ê°€ëŠ¥í•œ í•­ëª©ì´ ë§Žì€ ê²½ìš° ìš”ì•½ì´ ê¸¸ì–´ì ¸ë„ ê´œì°®ìŠµë‹ˆë‹¤.
â€» í•­ëª©ì´ ì—†ìœ¼ë©´ ìƒëžµí•´ë„ ë©ë‹ˆë‹¤.

\n\n{chunk}"""
    try:
        response = llm_client.chat.completions.create(
            model="solar-pro",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ íšŒê³„ ë¬¸ì„œë¥¼ ìš”ì•½í•  ë•Œ, í•µì‹¬ íšŒê³„ ì´ìŠˆë¥¼ ë†“ì¹˜ì§€ ì•ŠëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[ìš”ì•½ ì‹¤íŒ¨] {e}")
        return "[ìš”ì•½ ì‹¤íŒ¨]"


def summarize_in_chunks_parallel(text: str, max_workers: int = 6) -> list:
    chunks = split_text_by_length(text)
    print(f"[âš¡] {len(chunks)}ê°œì˜ chunkë¥¼ ë³‘ë ¬ë¡œ ìš”ì•½ ì¤‘...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        summaries = list(executor.map(summarize_chunk, chunks))
    return summaries


def merge_summaries(summaries: list) -> str:
    combined = "\n\n".join(summaries)
    prompt = f"""ë‹¤ìŒì€ íšŒê³„ ë³´ê³ ì„œë¥¼ ë‚˜ëˆ ì„œ ìš”ì•½í•œ ë‚´ìš©ìž…ë‹ˆë‹¤. ì´ë“¤ì„ í•˜ë‚˜ì˜ ìµœì¢… ìš”ì•½ìœ¼ë¡œ í†µí•©í•´ ì£¼ì„¸ìš”:\n\n{combined}"""
    try:
        response = llm_client.chat.completions.create(
            model="solar-pro",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” íšŒê³„ ë³´ê³ ì„œë¥¼ í†µí•© ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[í†µí•© ìš”ì•½ ì‹¤íŒ¨] {e}")
        return "[í†µí•© ìš”ì•½ ì‹¤íŒ¨]"


def summarize_pdf_fully(pdf_path: str) -> str: #full pipeline
    print(f"[ðŸ“„] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {pdf_path}")
    raw_text = extract_text_from_pdf(pdf_path)
    print(f"[âœ‚ï¸] í…ìŠ¤íŠ¸ ë¶„í•  + ë³‘ë ¬ ìš”ì•½ ì¤‘...")
    chunk_summaries = summarize_in_chunks_parallel(raw_text, max_workers=6)
    print(f"[ðŸ§ ] í†µí•© ìš”ì•½ ìƒì„± ì¤‘...")
    final_summary = merge_summaries(chunk_summaries)
    return final_summary

# RAG AGI pipeline
def classify_task_with_llm(task_prompt: str) -> str:
    classification_prompt = f"""
ìš°ë¦¬ê°€ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” TASKëŠ” ì˜¤ì§ í•œ ê°€ì§€ìž…ë‹ˆë‹¤:

"ìœ ì‚¬í•œ ë‹¤ë¥¸ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ê²€ìƒ‰"

ì‚¬ìš©ìžì˜ ìš”ì²­: {task_prompt}

"YES" ë˜ëŠ” "None"ìœ¼ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
- "ìœ ì‚¬í•œ ë‹¤ë¥¸ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ê²€ìƒ‰"ì— í•´ë‹¹ëœë‹¤ë©´, "YES"ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.
- "ìœ ì‚¬í•œ ë‹¤ë¥¸ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ê²€ìƒ‰"ì— í•´ë‹¹ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´, "None"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
"""
    response = llm_client.chat.completions.create(
        model="solar-pro",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¶„ë¥˜ë¥¼ ë„ì™€ì£¼ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤."},
            {"role": "user", "content": classification_prompt}
        ],
        stream=False
    )
    classification = response.choices[0].message.content.strip()
    if classification not in ["1", "2"]:
        classification = "None"
    return classification

def retrieve_qdrant_docs_by_ids(qdrant_client, collection_name: str, doc_ids: list):
    retrieved = qdrant_client.retrieve(
        collection_name=collection_name,
        ids=doc_ids
    )
    return retrieved

def search_and_rerank_with_indexing(qdrant_client, co, query_vector, collection_name):
    results = qdrant_client.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=5
    )
    candidates = []
    for r in results:
        candidates.append({
            "id": r.id,
            "score": r.score,
            "payload": r.payload
        })

    documents = [c["payload"]["text"] for c in candidates]
    rerank_response = co.rerank(
        query="ìœ ì‚¬í•œ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ì°¾ê¸°",
        documents=documents,
        top_n=len(documents),
        model="rerank-multilingual-v3.0"
    )
    reranked_indices = sorted(
        rerank_response.results,
        key=lambda x: x.relevance_score,
        reverse=True
    )

    reranked_candidates = []
    for i, r in enumerate(reranked_indices):
        original_idx = r.index
        reranked_candidates.append({
            "report_index": i,
            "id": candidates[original_idx]["id"],
            "score": candidates[original_idx]["score"],
            "payload": candidates[original_idx]["payload"]
        })
    return reranked_candidates

def retrieve_by_report_index(report_index: int, reranked_candidates, qdrant_client, collection_name):
    match_candidates = [c for c in reranked_candidates if c["report_index"] == report_index]
    if not match_candidates:
        print(f"[ì—­ì¶”ì  ì‹¤íŒ¨] report_index={report_index} ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    qdrant_id = match_candidates[0]["id"]
    print(f"[ì—­ì¶”ì ] report_index={report_index} â†’ Qdrant ID={qdrant_id}")

    retrieved_docs = retrieve_qdrant_docs_by_ids(qdrant_client, collection_name, [qdrant_id])
    for doc in retrieved_docs:
        print(f"[ì—­ì¶”ì  ê²°ê³¼] ID={doc.id}, payload={doc.payload}")
        return doc
    return None

def get_solar_embedding(text: str) -> List[float]:
    response = llm_client.embeddings.create(
        input=text,
        model="embedding-query"
    )
    embedding_vector = response.data[0].embedding
    return embedding_vector

def generate_final_answer_with_llm(query: str, contexts: List[str]) -> str:
    combined_context = "\n".join(contexts)
    system_prompt = (
        "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ íšŒê³„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ëŠ” í•˜ë‚˜ì˜ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œì™€ ìœ ì‚¬í•œ ë³´ê³ ì„œë“¤ì˜ ìš”ì•½ìž…ë‹ˆë‹¤. "
        "ì´ í…ìŠ¤íŠ¸ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì–´ë–¤ ë³´ê³ ì„œê°€ ê°€ìž¥ ìœ ì‚¬í•œì§€ íŒë‹¨í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ìƒì„¸ížˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n\n"
        "ìœ ì‚¬ì„± íŒë‹¨ ê¸°ì¤€ì—ëŠ” ë‹¤ìŒ ìš”ì†Œë“¤ì„ ê³ ë ¤í•˜ì„¸ìš”:\n"
        "- íšŒê³„ê¸°ì¤€ ì ìš© (K-IFRS, K-GAAP ë“±)\n"
        "- ìˆ˜ìµ ì¸ì‹ ë°©ì‹\n"
        "- ì¶©ë‹¹ë¶€ì±„ ì²˜ë¦¬ ë°©ì‹\n"
        "- ë¬´í˜•ìžì‚° ì¸ì‹ ë° ìžì‚°í™” ì—¬ë¶€\n"
        "- ê°ê°€ìƒê° ë°©ë²• ë° ë‚´ìš©ì—°ìˆ˜\n\n"
        "ìµœì¢… ì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:"
    )
    user_prompt = f"""
[ë³´ê³ ì„œ ì»¨í…ìŠ¤íŠ¸]
{combined_context}

[ì‚¬ìš©ìž ì§ˆë¬¸]
{query}

[ìš”ì²­]
- ìž…ë ¥ ë³´ê³ ì„œì™€ ê°€ìž¥ ìœ ì‚¬í•œ ë³´ê³ ì„œë¥¼ 1ê°œ ì„ íƒí•˜ê³ , ê·¸ ìœ ì‚¬í•œ ì´ìœ ë¥¼ ëª…í™•ížˆ ìž‘ì„±í•´ ì£¼ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
{{
  "ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID": "ë³´ê³ ì„œ 4",
  "ìœ ì‚¬í•œ_ì´ìœ ": "ë‘ ë³´ê³ ì„œëŠ” ëª¨ë‘ K-IFRSë¥¼ ì ìš©í•˜ê³  ìžˆìœ¼ë©°, ìˆ˜ìµ ì¸ì‹, ì¶©ë‹¹ë¶€ì±„, ë¬´í˜•ìžì‚° íšŒê³„ì²˜ë¦¬ ë°©ì‹ì´ ìœ ì‚¬í•©ë‹ˆë‹¤."
}}
"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    response = llm_client.chat.completions.create(
        model="solar-pro",
        messages=messages,
        stream=False,
    )
    return response.choices[0].message.content

def postprocess_final_answer_with_company_name(final_answer: str, reranked_candidates: list) -> str:
    """
    LLM ê²°ê³¼ì˜ "ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID": "ë³´ê³ ì„œ 3" â†’ ì‹¤ì œ filename ë“±ìœ¼ë¡œ ë°”ê¾¸ëŠ” í•¨ìˆ˜
    """
    try:
        parsed = json.loads(final_answer)
        report_id = parsed.get("ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID", "")
        if report_id.startswith("ë³´ê³ ì„œ "):
            report_index = int(report_id.replace("ë³´ê³ ì„œ ", ""))
            for c in reranked_candidates:
                if c["report_index"] == report_index:
                    fname = c["payload"].get("filename", "")
                    match = re.search(r"^(\[.*?\].*?\(\d{4}\.\d{2}\.\d{2}\))", fname)
                    if match:
                        extracted = match.group(1)
                        parsed["ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID"] = extracted
                        final_answer = json.dumps(parsed, ensure_ascii=False)
                        break
    except Exception as e:
        print(f"[í›„ì²˜ë¦¬ ì˜¤ë¥˜] {e}")
    return final_answer

def rag_based_agi_pipeline(task_prompt: str, report_1: str, report_2: str = "") -> str:
    task_type = classify_task_with_llm(task_prompt)
    if task_type == "YES":
        query = "ìœ ì‚¬í•œ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ì°¾ê¸°"
        query_vector = get_solar_embedding(query)
        reranked_candidates = search_and_rerank_with_indexing(qdrant_client, co, query_vector, COLLECTION_NAME)
        top_contexts = [item["payload"]["text"] for item in reranked_candidates[:3]]
        final_answer = generate_final_answer_with_llm(query, top_contexts)

        final_answer = postprocess_final_answer_with_company_name(final_answer, reranked_candidates)

        final_answer = re.sub(r"[{}]", "", final_answer)

        return final_answer

    else:
        return "ìš”ì²­í•œ TASKë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
